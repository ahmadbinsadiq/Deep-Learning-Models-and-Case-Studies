{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP)\n",
    "\n",
    "## Introduction\n",
    "Natural Language Processing (NLP) is a field of artificial intelligence that deals with the interaction between computers and humans using natural language. The ultimate objective of NLP is to read, decipher, understand, and make sense of the human languages in a valuable way. Most NLP techniques rely on machine learning to derive meaning from human languages.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Key Concepts](#1)\n",
    "\n",
    "\n",
    "## Key Concepts <a id=\"1\"></a>\n",
    "| Concept | Description | Example |\n",
    "| --- | --- | --- |\n",
    "| Tokenization | Tokenization is the process of breaking down text into words, phrases, symbols, or other meaningful elements. | The quick brown fox jumps over the lazy dog. -> ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.'] |\n",
    "| Stemming | Stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base, or root form. | running, runs, ran -> run |\n",
    "| Lemmatization | Lemmatization is the process of grouping together the inflected forms of a word so they can be analysed as a single item. | good, better, best -> good, good, good |\n",
    "| Stop Words | Stop words are the most common words in a language that are often filtered out before or after processing of text. | a, an, the, in, on, at, to, from, of |\n",
    "| Bag of Words | Bag of Words is a method to represent text data when modeling text with machine learning algorithms. | The quick brown fox jumps over the lazy dog. -> [1, 2, 3, 4, 5, 6, 1, 7, 8, 9] |\n",
    "| TF-IDF | Term Frequency-Inverse Document Frequency (TF-IDF) is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. | TF = (Number of times term t appears in a document) / (Total number of terms in the document) |\n",
    "| NER | Named Entity Recognition (NER) is a subtask of information extraction that seeks to locate and classify named entities in text into pre-defined categories such as the names of persons, organizations, locations, expressions of times, quantities, monetary values, percentages, etc. | Barack Obama was born in Hawaii. -> Barack Obama (Person), Hawaii (Location) |\n",
    "| POS | Part-of-Speech (POS) tagging is the process of marking up a word in a text as corresponding to a particular part of speech, based on both its definition and its context. | The quick brown fox jumps over the lazy dog. -> The (Determiner), quick (Adjective), brown (Adjective), fox (Noun), jumps (Verb), over (Preposition), the (Determiner), lazy (Adjective), dog (Noun) |\n",
    "| Syntax and Parsing | Syntax is the set of rules, principles, and processes that govern the structure of sentences in a given language, usually including word order. Parsing is the process of analyzing a string of symbols, either in natural language or in computer languages, conforming to the rules of a formal grammar. | The quick brown fox jumps over the lazy dog. -> [The [quick [brown [fox [jumps [over [the [lazy dog]]]]]]]] |\n",
    "| Semantic Analysis | Semantic analysis is the process of understanding the meaning of words, phrases, and sentences. | Apple is a company. -> Apple (Organization) |\n",
    "| Sentiment Analysis | Sentiment analysis is the process of determining the sentiment of a piece of text. | I love this product! -> Positive |\n",
    "| Word Embedding | Word Embedding is the collective name for a set of language modeling and feature learning techniques in natural language processing where words or phrases from the vocabulary are mapped to vectors of real numbers. | ['king', 'man', 'queen', 'woman'] -> relation(king and man) = relation(queen and woman) |\n",
    "| Sequence-to-Sequence | Sequence-to-Sequence is a model that converts sequences from one domain to sequences in another domain. | English: What is your name? -> French: Quel est votre nom? |\n",
    "| Transformer | Transformer can be explained as a deep learning model that adopts the mechanism of self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution. | you  can  do  it -> [0.1, 0.2, 0.3, 0.4] |\n",
    "| Language Model | Language Model is a statistical model that is used to predict the probability of a word given the previous words in the sequence. | The quick brown fox jumps______ -> P( over the lazy dog.) |\n",
    "| BERT | BERT (Bidirectional Encoder Representations from Transformers) gives the model the context to understand the meaning of a word in a sentence by looking at the words that come before and after it. | I made a bank deposit -> I made a river bank deposit |\n",
    "| GPT | GPT (Generative Pre-trained Transformer) is a type of language model that is trained on a large corpus of text data in an unsupervised manner. | The quick brown fox jumps over the lazy dog. -> The quick brown fox jumps over the lazy dog. |\n",
    "| Music LLAMA | Music LLAMA (Language Model for Music Analysis) is a language model that is trained on a large corpus of music data to generate music. | [C, G, Am, F] -> [C, G, Am, F] |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
